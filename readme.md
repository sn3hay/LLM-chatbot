# RAG Chatbot with ChromaDB & Ollama LLM

## ğŸ“Œ Overview
This project is a **Retrieval-Augmented Generation (RAG) chatbot** that allows users to query a **PDF document** using a locally running **Ollama LLM**. It utilizes **ChromaDB** for vector storage and retrieval, enabling efficient document search.

## ğŸ— Project Structure
```
LLM-chatbot/
â”‚â”€â”€ chat.py             # Streamlit UI and chatbot logic
â”‚â”€â”€ data_handler.py     # Handles PDF ingestion and ChromaDB operations
â”‚â”€â”€ query_llm.py        # Queries the local Ollama LLM with retrieved text
â”‚â”€â”€ .venv/              # Virtual environment (optional)
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ README.md           # Project documentation
```

## âš™ï¸ Features
- **Extracts text from PDFs** and stores vector embeddings in **ChromaDB**.
- **Retrieves relevant text** chunks based on user queries.
- **Uses Ollama LLM** for intelligent responses.
- **Streamlit UI** for easy interaction.

## ğŸš€ Installation & Setup
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/sn3hay/LLM-chatbot.git
cd LLM-chatbot
```

### 2ï¸âƒ£ Create & Activate Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
.venv\Scripts\activate    # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Chatbot
```bash
streamlit run chat.py
```

## ğŸ— How It Works
1. **PDF Processing**: Extracts text from uploaded PDFs.
2. **Vector Storage**: Converts text into embeddings & stores them in **ChromaDB**.
3. **Query Handling**: Uses **similarity search** to retrieve relevant text.
4. **LLM Response**: Sends retrieved text to **Ollama LLM** for answering user queries.

## ğŸ–¼ Screenshots of Outcome
Here are some screenshots of the chatbot in action:

![Screenshot 1](output-images/screenshots-1.png)
*Response given by the chatbot.*

![Screenshot 2](output-images/screenshots-2.png)
*Response given by the chatbot.*

## ğŸ›  Configuration
- Ensure **Ollama LLM** is running locally.
- Adjust ChromaDB parameters as needed in `data_handler.py`.

